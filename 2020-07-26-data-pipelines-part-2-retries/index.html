<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=x-ua-compatible content="IE=edge, chrome=1"><title>Data Pipelines: Part 2 - Retries | Go (Golang) and Data Programming Blog - Nick Poorman</title><meta name=Description content="Building a cloud-scale central retry service for Data Pipelines."><meta property="og:title" content="Data Pipelines: Part 2 - Retries"><meta property="og:description" content="Building a cloud-scale central retry service for Data Pipelines."><meta property="og:type" content="article"><meta property="og:url" content="https://nickpoorman.com/2020-07-26-data-pipelines-part-2-retries/"><meta property="og:image" content="https://nickpoorman.com/cover.png"><meta property="article:published_time" content="2020-07-26T12:00:00-05:00"><meta property="article:modified_time" content="2020-07-27T12:09:30-05:00"><meta property="og:site_name" content="Go (Golang) and Data Programming Blog - Nick Poorman"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nickpoorman.com/cover.png"><meta name=twitter:title content="Data Pipelines: Part 2 - Retries"><meta name=twitter:description content="Building a cloud-scale central retry service for Data Pipelines."><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=canonical href=https://nickpoorman.com/2020-07-26-data-pipelines-part-2-retries/><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=prev href=https://nickpoorman.com/2020-07-26-data-pipelines-part-1-queuing-and-messaging-patterns/><link rel=stylesheet href=/css/style.min.min.6dddeb33532c40abe7cb958a84fb11dd7bec4f445f7b3f7dd1ec28e82974ff821055f15d1f4cc9387bf35326f0c7d46bec33e41df099253a7b19b5324fd44df0.css integrity="sha512-bd3rM1MsQKvny5WKhPsR3XvsT0Rfez990ewo6Cl0/4IQVfFdH0zJOHvzUybwx9Rr7DPkHfCZJTp7GbUyT9RN8A=="><link rel=preload href=/css/lib/fontawesome-free/all.min.min.c10b72e627916929312e76aae32cd871083b1a98e9603625bd9f45aedfa7fd382b9fe588affc4417ecfd42357214e2b1bdccd180b10a9f2bfc1dc42fdc3a6fd5.css integrity="sha512-wQty5ieRaSkxLnaq4yzYcQg7GpjpYDYlvZ9Frt+n/Tgrn+WIr/xEF+z9QjVyFOKxvczRgLEKnyv8HcQv3Dpv1Q==" as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/lib/fontawesome-free/all.min.min.c10b72e627916929312e76aae32cd871083b1a98e9603625bd9f45aedfa7fd382b9fe588affc4417ecfd42357214e2b1bdccd180b10a9f2bfc1dc42fdc3a6fd5.css integrity="sha512-wQty5ieRaSkxLnaq4yzYcQg7GpjpYDYlvZ9Frt+n/Tgrn+WIr/xEF+z9QjVyFOKxvczRgLEKnyv8HcQv3Dpv1Q=="></noscript><link rel=preload href=/css/lib/lightbox/lightbox.min.min.b999b8d703e46e94fdff333f191b7fbe43aea06dc495e4b8243060e495f4c42bba68935cdcfd0a02a4b843102ae909feebfb244a01f82ce3c17ec00a778a5dba.css integrity="sha512-uZm41wPkbpT9/zM/GRt/vkOuoG3EleS4JDBg5JX0xCu6aJNc3P0KAqS4QxAq6Qn+6/skSgH4LOPBfsAKd4pdug==" as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/lib/lightbox/lightbox.min.min.b999b8d703e46e94fdff333f191b7fbe43aea06dc495e4b8243060e495f4c42bba68935cdcfd0a02a4b843102ae909feebfb244a01f82ce3c17ec00a778a5dba.css integrity="sha512-uZm41wPkbpT9/zM/GRt/vkOuoG3EleS4JDBg5JX0xCu6aJNc3P0KAqS4QxAq6Qn+6/skSgH4LOPBfsAKd4pdug=="></noscript><meta name=p:domain_verify content="e820434c419f4c0782c7916de63ecb60"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Data Pipelines: Part 2 - Retries","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/nickpoorman.com\/2020-07-26-data-pipelines-part-2-retries\/"},"image":{"@type":"ImageObject","url":"https:\/\/nickpoorman.com\/cover.png","width":1200,"height":628},"genre":"posts","keywords":"Golang, Data, Queuing, Messaging Patterns, NATS, gRPC, Retries","wordcount":3271,"url":"https:\/\/nickpoorman.com\/2020-07-26-data-pipelines-part-2-retries\/","datePublished":"2020-07-26T12:00:00-05:00","dateModified":"2020-07-27T12:09:30-05:00","publisher":{"@type":"Organization","name":"Nick Poorman","logo":{"@type":"ImageObject","url":"https:\/\/nickpoorman.com\/logo.png","width":127,"height":40}},"description":"Building a cloud-scale central retry service for Data Pipelines."}</script></head><body><script>if(!window.localStorage||!window.localStorage.getItem('theme')){window.isDark=window.matchMedia('(prefers-color-scheme: dark)').matches;}else{window.isDark=(window.localStorage&&window.localStorage.getItem('theme'))==='dark';}
window.isDark&&document.body.classList.add('dark-theme');</script><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/>Intersection of Go and Data</a></div><div class=menu><a class=menu-item href=/posts/ title="list of blog posts">Posts</a><a class=menu-item href=/tags/ title="list of article tags">Tags</a><a class=menu-item href=/about/ title="Nick Poorman about page">About</a><a href=javascript:void(0); class=theme-switch title="Switch website theme to Dark mode" aria-label="Switch website theme to Dark mode">
<i class="fas fa-adjust fa-rotate-180 fa-fw"></i></a></div></div></header><header class=mobile id=header-mobile><div class=header-wrapper><div class=header-container><div class=header-title><a href=/>Intersection of Go and Data</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title="list of blog posts">Posts</a><a class=menu-item href=/tags/ title="list of article tags">Tags</a><a class=menu-item href=/about/ title="Nick Poorman about page">About</a><a href=javascript:void(0); class=theme-switch title="Switch website theme to Dark mode" aria-label="Switch website theme to Dark mode">
<i class="fas fa-adjust fa-rotate-180 fa-fw"></i></a></div></div></header><script>window.desktopHeaderMode="fixed";window.mobileHeaderMode="auto";</script><main class=main><div class=container><article class="page single"><h1 class=single-title>Data Pipelines: Part 2 - Retries</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a class=author href=/ rel=author target=_blank><i class="fas fa-user-circle fa-fw"></i>Nick Poorman</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i><time datetime=2020-07-26>2020-07-26</time>&nbsp;
<i class="fas fa-pencil-alt fa-fw"></i>about 3271 words&nbsp;
<i class="far fa-clock fa-fw"></i>16 min&nbsp;</div></div><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><div class=toc id=toc-static><details><summary><div class=toc-title><span>Contents</span>
<span><i class="details icon fas fa-angle-down"></i></span></div></summary><div class=toc-content id=toc-content-static><nav id=TableOfContents><ul><li><ul><li><a href=#a-different-approach>A Different Approach</a></li><li><a href=#nats-as-a-message-relay>NATS as a Message Relay</a></li><li><a href=#adding-downstream-consumers>Adding Downstream Consumers</a></li><li><a href=#acknowledgments>Acknowledgments</a></li><li><a href=#design-by-requirements>Design by Requirements</a></li><li><a href=#decoupling>Decoupling</a></li><li><a href=#enhancements>Enhancements</a></li><li><a href=#summary>Summary</a></li></ul></li></ul></nav></div></details></div><div class=content><h3 id=a-different-approach>A Different Approach</h3><p>In the last post <a href=/2020-07-26-data-pipelines-part-1-queuing-and-messaging-patterns/ rel="noopener noreffer">&ldquo;Data Pipelines: Part 1 - Queuing and Messaging Patterns&rdquo;</a>
, I discussed the various messaging patterns such as pub-sub and push-pull. I also discussed various queueing storage mechanisms such as in-memory and on disk. The last post left off with a solution for reliably sending messages to a webhook that could possibly experience downtime, or have rate limiting requirements. In this post, I will discuss how we can wire a pub-sub technology to a storage engine to solve some of the issues surrounding downtime, rate limiting, backpressure, retry, and dead letter queues. For demonstration purposes, I will continue on with our totaling of page views example used in the previous post in this series.</p><h3 id=nats-as-a-message-relay>NATS as a Message Relay</h3><p>For the purpose of this post, I will be using a popular open-source messaging system called <a href=https://nats.io/ target=_blank rel="noopener noreffer">NATS</a>
. Specifically, the code examples will be using the <a href=https://github.com/nats-io/nats.go target=_blank rel="noopener noreffer">Go implementation for NATS</a>
—although there are <a href=https://nats.io/download/ target=_blank rel="noopener noreffer">implementations in most other popular languages</a>
.</p><p>NATS has two main features we will be using. The first is <code>Publish</code>, which takes a subject, also known as a topic, in which to publish messages to, and a message payload.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=c1>// Simple Publisher
</span><span class=c1>// subject is foo and the paylod is the &#34;Hello World&#34; bytes.
</span><span class=c1></span><span class=nx>nc</span><span class=p>.</span><span class=nf>Publish</span><span class=p>(</span><span class=s>&#34;foo&#34;</span><span class=p>,</span> <span class=p>[]</span><span class=nb>byte</span><span class=p>(</span><span class=s>&#34;Hello World&#34;</span><span class=p>))</span>
</code></pre></td></tr></table></div></div><p>The second is feature is <code>Subscribe</code>, which takes a subject to listen for messages on and a function to call for each message that is received on that subject.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=c1>// Simple Async Subscriber
</span><span class=c1></span><span class=nx>nc</span><span class=p>.</span><span class=nf>Subscribe</span><span class=p>(</span><span class=s>&#34;foo&#34;</span><span class=p>,</span> <span class=kd>func</span><span class=p>(</span><span class=nx>m</span> <span class=o>*</span><span class=nx>nats</span><span class=p>.</span><span class=nx>Msg</span><span class=p>)</span> <span class=p>{</span>
    <span class=nx>fmt</span><span class=p>.</span><span class=nf>Printf</span><span class=p>(</span><span class=s>&#34;Received a message: %s\n&#34;</span><span class=p>,</span> <span class=nb>string</span><span class=p>(</span><span class=nx>m</span><span class=p>.</span><span class=nx>Data</span><span class=p>))</span>
<span class=p>})</span>
</code></pre></td></tr></table></div></div><p>When a message is published in NATS it goes though a NATS server and is relayed to any subscribers listening to that subject. When the subscriber receives a message the function we provide, will be called asynchronously and will include the message that was received. Let&rsquo;s look at some code for the totaling of page views from the previous post in this series.</p><p>Below is an example of a consumer responsible for totaling up page views.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=c1>// Connect to the NATS server
</span><span class=c1></span><span class=nx>nc</span><span class=p>,</span> <span class=nx>_</span> <span class=o>:=</span> <span class=nx>nats</span><span class=p>.</span><span class=nf>Connect</span><span class=p>(</span><span class=nx>nats</span><span class=p>.</span><span class=nx>DefaultURL</span><span class=p>)</span>

<span class=kd>var</span> <span class=nx>totalPageViews</span> <span class=kt>int64</span>

<span class=c1>// Subscribe to page_view messages from our users.
</span><span class=c1></span><span class=nx>nc</span><span class=p>.</span><span class=nf>Subscribe</span><span class=p>(</span><span class=s>&#34;page_views&#34;</span><span class=p>,</span> <span class=kd>func</span><span class=p>(</span><span class=nx>m</span> <span class=o>*</span><span class=nx>nats</span><span class=p>.</span><span class=nx>Msg</span><span class=p>)</span> <span class=p>{</span>
    <span class=nx>fmt</span><span class=p>.</span><span class=nf>Printf</span><span class=p>(</span><span class=s>&#34;Received a page view from a user: %s\n&#34;</span><span class=p>,</span> <span class=nb>string</span><span class=p>(</span><span class=nx>m</span><span class=p>.</span><span class=nx>Data</span><span class=p>))</span>

    <span class=c1>// Increment the number of page views.
</span><span class=c1></span>    <span class=nx>atomic</span><span class=p>.</span><span class=nf>AddInt64</span><span class=p>(</span><span class=o>&amp;</span><span class=nx>totalPageViews</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=c1>// i.e., totalPageViews++
</span><span class=c1></span><span class=p>})</span>
</code></pre></td></tr></table></div></div><p>Next, we&rsquo;ll create an example of what a user creating a page view message might look like.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=c1>// Connect to the NATS server
</span><span class=c1></span><span class=nx>nc</span><span class=p>,</span> <span class=nx>_</span> <span class=o>:=</span> <span class=nx>nats</span><span class=p>.</span><span class=nf>Connect</span><span class=p>(</span><span class=nx>nats</span><span class=p>.</span><span class=nx>DefaultURL</span><span class=p>)</span>

<span class=nx>msg</span> <span class=o>:=</span> <span class=err>&#39;</span><span class=p>{</span><span class=s>&#34;visitTimestamp&#34;</span><span class=p>:</span><span class=s>&#34;2020-07-14T18:47:13.919Z&#34;</span><span class=p>,</span><span class=s>&#34;page&#34;</span><span class=p>:</span><span class=s>&#34;/blog/data-pipeline-queuing-and-messaging-patterns&#34;</span><span class=p>,</span><span class=s>&#34;publishedTimestamp&#34;</span><span class=p>:</span><span class=s>&#34;2020-07-14T08:00:00.000Z&#34;</span><span class=p>}</span><span class=err>&#39;</span>

<span class=c1>// User publishes a page_view message
</span><span class=c1></span><span class=nx>nc</span><span class=p>.</span><span class=nf>Publish</span><span class=p>(</span><span class=s>&#34;page_views&#34;</span><span class=p>,</span> <span class=p>[]</span><span class=nb>byte</span><span class=p>(</span><span class=nx>msg</span><span class=p>))</span>
</code></pre></td></tr></table></div></div><p>In the above example, you can see how our consumer subscribes to a <code>page_view</code> subject, and when it receives a message from our producer, we increment the total number of page views.</p><p>In reality, if our users are visiting our website, we might have an API that users make requests to from JavaScript. That API would then be responsible for publishing the page view message over NATS.</p><h3 id=adding-downstream-consumers>Adding Downstream Consumers</h3><p>As you may recall in the last post of this series, we walked through a solution for dealing with downtime. In our example, we had a webhook which we were required to send messages to. The webhook was controlled by an analytics SaaS company. We needed to guarantee that our messages would eventually make it to the analytics SaaS company in the event that the analytics webhook was offline for a period of time. We solved this by placing a Streaming technology between our producer and the downstream analytics webhook. This allowed us to control the rate at which we sent messages to the webhook and in doing so, allowed us to pause sending messages to the webhook while it was down.</p><p><figure><img src=/svg/loading.min.svg data-sizes=auto data-src=saas-webhook-streaming.svg alt="Diagram of a SaaS Webhook With Streaming" title="Diagram of a SaaS Webhook With Streaming" class=lazyload><figcaption class=image-caption>Diagram of a SaaS Webhook With Streaming</figcaption></figure></p><p>With that solution in place, we don&rsquo;t have to worry about downtime or rate limiting by the SaaS webhook. But that solution has a scaling flaw. What if we need to send our messages to additional SaaS webhooks? Does that mean we need to spin up additional infrastructure for every additional webhook? Some people would say no. Those people might be using a streaming technology such as Kafka that allows users to produce and consume messages on different topics. Let&rsquo;s say for the sake of this post that we don&rsquo;t have a <a href="https://www.google.com/search?hl=en&sxsrf=ALeKk01o5ijLDYNKn5ar_Sk1H6mbkdrRrg%3A1595436486931&ei=xm0YX6CsOITctQaW06jQAg&q=kafka+post+mortem&oq=kafka+post+mor&gs_lcp=CgZwc3ktYWIQAxgAMgUIIRCgATIFCCEQqwIyBQghEKsCOgQIABBHOgQIABBDOgUIABCRAjoFCC4QkQI6CAguELEDEIMBOgsILhCxAxDHARCjAjoFCC4QsQM6CAgAELEDEIMBOgIILjoECC4QQzoCCAA6CAgAELEDEJECOggILhCxAxCRAjoFCAAQsQM6BwgAELEDEEM6BggAEBYQHjoICAAQFhAKEB5QxaoBWITGAWDc0AFoAHADeACAAYsBiAGUC5IBAzcuN5gBAKABAaoBB2d3cy13aXrAAQE&sclient=psy-ab" target=_blank rel="noopener noreffer">24-hour SRE team at the ready who can manage our Kafka cluster for us</a>
. Maybe we&rsquo;re using a streaming technology that doesn&rsquo;t have a concept of topics or subjects like <a href=https://aws.amazon.com/kinesis/ target=_blank rel="noopener noreffer">AWS Kinesis</a>
.</p><p>Let&rsquo;s think about some of our requirements here.</p><ol><li>We need the ability to add more webhooks without manually wiring up new infrastructure.</li><li>We need to ensure the messages eventually make it to our webhooks. Our solution should be resilient to webhook downtime. It should also be able to deal with rate limit requirements a given webhook might have.</li><li>We want our messages committed to disk. We will most likely have more data than can be reasonably kept in RAM. Also, committing messages to disk gives us better message durability guarantees should the system restart.</li><li>We don&rsquo;t want the overhead of managing a large replicated service like Kafka.</li><li>We need acknowledgment capabilities between every producer, consumer, and webhook.</li></ol><h3 id=acknowledgments>Acknowledgments</h3><p>Before we move on, we need to go back to the last post where we talked about acknowledgments. When sending a message from a producer to a consumer, we need a way for the consumer to acknowledge that the message was received and processed. Let&rsquo;s use the case of consumer-2 and SaaS Webhook in the above diagram. When consumer-2 makes an HTTP Request to SaaS Webhook, SaaS Webhook will accept and process the message. Once the message has been processed, SaaS Webhook will respond with an acknowledgment. In the case of HTTP, the acknowledgment is probably an HTTP 200 response code. Internally, we are using pub-sub so our acknowledgments should be similar. A producer will publish a message and a consumer should respond with an acknowledgment once it has processed the message. We can do this quite simply in NATS.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=c1>// Responding to a request message
</span><span class=c1></span><span class=nx>nc</span><span class=p>.</span><span class=nf>Subscribe</span><span class=p>(</span><span class=s>&#34;request&#34;</span><span class=p>,</span> <span class=kd>func</span><span class=p>(</span><span class=nx>m</span> <span class=o>*</span><span class=nx>nats</span><span class=p>.</span><span class=nx>Msg</span><span class=p>)</span> <span class=p>{</span>
    <span class=nx>m</span><span class=p>.</span><span class=nf>Respond</span><span class=p>([]</span><span class=nb>byte</span><span class=p>(</span><span class=s>&#34;The answer is 42.&#34;</span><span class=p>))</span>
<span class=p>})</span>

<span class=c1>// Publish a message
</span><span class=c1></span><span class=nx>nc</span><span class=p>.</span><span class=nf>Request</span><span class=p>(</span><span class=s>&#34;request&#34;</span><span class=p>,</span> <span class=p>[]</span><span class=nb>byte</span><span class=p>(</span><span class=s>&#34;What is the answer to the Ultimate Question of Life, the Universe, and Everything?&#34;</span><span class=p>))</span>
</code></pre></td></tr></table></div></div><p>NATS makes it easy to <a href=https://docs.nats.io/nats-concepts/reqreply target=_blank rel="noopener noreffer">respond to a request</a>
. Any time you make a request, it sends the request to only one subscriber in a <a href=https://docs.nats.io/nats-concepts/queue target=_blank rel="noopener noreffer">queue group</a>
. Along with the request, it includes an inbox address where the response should be sent to. The inbox address is simply another subject specifically created to receive a response for that request. Using the <code>Request</code> and <code>Respond</code> we can satisfy our acknowledgments requirement. We&rsquo;ll also give the consumer 30 seconds to respond with an acknowledgment before the request times out and results in an error.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=c1>// Responding to a request message
</span><span class=c1></span><span class=nx>nc</span><span class=p>.</span><span class=nf>Subscribe</span><span class=p>(</span><span class=s>&#34;page_views&#34;</span><span class=p>,</span> <span class=kd>func</span><span class=p>(</span><span class=nx>m</span> <span class=o>*</span><span class=nx>nats</span><span class=p>.</span><span class=nx>Msg</span><span class=p>)</span> <span class=p>{</span>
    <span class=nx>m</span><span class=p>.</span><span class=nf>Respond</span><span class=p>([]</span><span class=nb>byte</span><span class=p>(</span><span class=s>&#34;Acknowledged&#34;</span><span class=p>))</span>
<span class=p>})</span>

<span class=nx>msg</span> <span class=o>:=</span> <span class=err>&#39;</span><span class=p>{</span><span class=s>&#34;visitTimestamp&#34;</span><span class=p>:</span><span class=s>&#34;2020-07-14T18:47:13.919Z&#34;</span><span class=p>,</span><span class=s>&#34;page&#34;</span><span class=p>:</span><span class=s>&#34;/blog/data-pipeline-queuing-and-messaging-patterns&#34;</span><span class=p>,</span><span class=s>&#34;publishedTimestamp&#34;</span><span class=p>:</span><span class=s>&#34;2020-07-14T08:00:00.000Z&#34;</span><span class=p>}</span><span class=err>&#39;</span>

<span class=c1>// Publish a message
</span><span class=c1></span><span class=nx>acknowledgmentMsg</span><span class=p>,</span> <span class=nx>err</span> <span class=o>:=</span> <span class=nx>nc</span><span class=p>.</span><span class=nf>Request</span><span class=p>(</span><span class=s>&#34;page_views&#34;</span><span class=p>,</span> <span class=p>[]</span><span class=nb>byte</span><span class=p>(</span><span class=nx>msg</span><span class=p>),</span> <span class=mi>30</span><span class=o>*</span><span class=nx>time</span><span class=p>.</span><span class=nx>Second</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><h3 id=design-by-requirements>Design by Requirements</h3><p>What if we thought about the problem slightly differently? What if, instead of putting a streaming technology such as Kinesis, which is really an iteration of a high-performance queue, in front of our webhooks, we replaced streaming-2,3,4&mldr;n with a single service? Let&rsquo;s design that service by going over our requirements one at a time.</p><p><em>Requirement #1 - We need the ability to add more webhooks without manually wiring up new infrastructure.</em></p><p>This requirement is something we should be able to solve with our NATS pub-sub technology. Since our pub-sub technology has the concept of subjects, we&rsquo;ll create a subject for each webhook. NATS specifically has the concept of <a href=https://docs.nats.io/nats-concepts/subjects target=_blank rel="noopener noreffer">subject hierarchies</a>
. Publishers can use the <code>.</code> character to create the subject hierarchy. We can utilize the subject hierarchies to create the subjects <code>webhooks.saas_webhook_1</code> and <code>webhooks.saas_webhook_2</code>. This way, we can create a single consumer that subscribes to all messages with the <code>webhooks.*</code> subject.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=c1>// Subscribe to all webhooks messages.
</span><span class=c1>// Note: We could also use &#34;webhooks.&gt;&#34; to match subjects with multiple tokens
</span><span class=c1>//  such as &#34;webhooks.saas_webhook_1.stats&#34;.
</span><span class=c1></span><span class=nx>nc</span><span class=p>.</span><span class=nf>Subscribe</span><span class=p>(</span><span class=s>&#34;webhooks.*&#34;</span><span class=p>,</span> <span class=kd>func</span><span class=p>(</span><span class=nx>msg</span> <span class=o>*</span><span class=nx>nats</span><span class=p>.</span><span class=nx>Msg</span><span class=p>)</span> <span class=p>{</span>
    <span class=k>switch</span> <span class=nx>wh</span> <span class=o>:=</span> <span class=nf>determineSaasWebhook</span><span class=p>(</span><span class=nx>msg</span><span class=p>)</span> <span class=p>{</span>
        <span class=k>case</span> <span class=nx>SaasWebhook1</span><span class=p>:</span>
            <span class=nf>sendToSaasWebhook1</span><span class=p>(</span><span class=nx>msg</span><span class=p>)</span>
        <span class=k>case</span> <span class=nx>SaaSWebhook2</span><span class=p>:</span>
            <span class=nf>sendToSaasWebhook2</span><span class=p>(</span><span class=nx>msg</span><span class=p>)</span>
        <span class=k>default</span><span class=p>:</span>
            <span class=nx>log</span><span class=p>.</span><span class=nf>Printf</span><span class=p>(</span><span class=s>&#34;unknown webhook: %s&#34;</span><span class=p>,</span> <span class=nx>wh</span><span class=p>)</span>
            <span class=c1>// Don&#39;t acknowledge.
</span><span class=c1></span>            <span class=k>return</span>
    <span class=p>}</span>
    <span class=nx>msg</span><span class=p>.</span><span class=nf>Respond</span><span class=p>([]</span><span class=nb>byte</span><span class=p>(</span><span class=s>&#34;Acknowledged&#34;</span><span class=p>))</span>
<span class=p>})</span>
</code></pre></td></tr></table></div></div><p><em>Requirement #2 - We need to ensure the messages eventually make it to our webhooks. Our solution should be resilient to webhook downtime. It should also be able to deal with rate limit requirements a given webhook might have.</em></p><p>Our system should accept messages on a subject as we established above for Requirement #1. The consumer should then forward the message to the webhook endpoint. <strong>When the webhook endpoint doesn&rsquo;t respond after a certain amount of time, we will say the message has timed out and send the message to a <a href=https://en.wikipedia.org/wiki/Dead_letter_queue target=_blank rel="noopener noreffer">dead letter queue</a>
.</strong> We could put this logic in the consumer of every webhook, but it might be better to generalize it and put it in the original publishers of the messages. Let&rsquo;s hold onto that thought for now as we go over the rest of the requirements.</p><p><em>Requirement #3 - We want our messages committed to disk. We will most likely have more data than can be reasonably kept in RAM. Also, committing messages to disk gives us better message durability guarantees should the system restart.</em></p><p>To keep things simple, I&rsquo;m choosing a persistent and fast key-value database called <a href=https://github.com/dgraph-io/badger target=_blank rel="noopener noreffer">Badger</a>
. Badger supports concurrent ACID transactions and serializable snapshot isolation guarantees, plus it&rsquo;s written in Go so we can easily embed it into the service we write with NATS. This will make our lives easier, especially when it comes to testing. With our chosen key-value store, we can solve this requirement by writing every message to disk (or batches of messages to disk), and once we have verified the message has been successfully committed to disk, respond to the original publisher with an acknowledgment.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=nx>db</span><span class=p>,</span> <span class=nx>err</span> <span class=o>:=</span> <span class=nx>badger</span><span class=p>.</span><span class=nf>Open</span><span class=p>(</span><span class=nx>badger</span><span class=p>.</span><span class=nf>DefaultOptions</span><span class=p>(</span><span class=s>&#34;/data/badger&#34;</span><span class=p>))</span>
<span class=c1>// ...
</span><span class=c1></span>
<span class=nx>nc</span><span class=p>.</span><span class=nf>Subscribe</span><span class=p>(</span><span class=s>&#34;page_views&#34;</span><span class=p>,</span> <span class=kd>func</span><span class=p>(</span><span class=nx>m</span> <span class=o>*</span><span class=nx>nats</span><span class=p>.</span><span class=nx>Msg</span><span class=p>)</span> <span class=p>{</span>
    <span class=nx>err</span> <span class=o>:=</span> <span class=nx>db</span><span class=p>.</span><span class=nf>Update</span><span class=p>(</span><span class=kd>func</span><span class=p>(</span><span class=nx>txn</span> <span class=o>*</span><span class=nx>badger</span><span class=p>.</span><span class=nx>Txn</span><span class=p>)</span> <span class=kt>error</span> <span class=p>{</span>
        <span class=nx>msgKey</span> <span class=o>:=</span> <span class=nx>ksuid</span><span class=p>.</span><span class=nf>New</span><span class=p>()</span> <span class=c1>// &#34;naturally&#34; sorted, globally unique identifier
</span><span class=c1></span>        <span class=nx>msgValue</span> <span class=o>:=</span> <span class=nx>m</span><span class=p>.</span><span class=nx>Data</span>
        <span class=nx>err</span> <span class=o>:=</span> <span class=nx>txn</span><span class=p>.</span><span class=nf>Set</span><span class=p>(</span><span class=nx>msgKey</span><span class=p>,</span> <span class=nx>msgValue</span><span class=p>)</span>
        <span class=k>return</span> <span class=nx>err</span>
    <span class=p>})</span>
    <span class=k>if</span> <span class=nx>err</span> <span class=o>!=</span> <span class=kc>nil</span> <span class=p>{</span>
        <span class=c1>// ... Don&#39;t acknowledge.
</span><span class=c1></span>    <span class=p>}</span>
    <span class=c1>// Message has been committed.
</span><span class=c1></span>    <span class=nx>m</span><span class=p>.</span><span class=nf>Respond</span><span class=p>([]</span><span class=nb>byte</span><span class=p>(</span><span class=s>&#34;Acknowledged&#34;</span><span class=p>))</span>
<span class=p>})</span>
</code></pre></td></tr></table></div></div><p><em>Requirement #4 - We don&rsquo;t want the overhead of managing a large replicated service like Kafka.</em></p><p>By using Badger, we are able to mount an NFS volume on our container. This could, for example, run in <a href=https://aws.amazon.com/ecs/ target=_blank rel="noopener noreffer">ECS</a>
with <a href=https://aws.amazon.com/efs/ target=_blank rel="noopener noreffer">EFS</a>
. The problem with ECS and EFS is that we don&rsquo;t have a <a href=https://github.com/aws/containers-roadmap/issues/127#issuecomment-656866073 target=_blank rel="noopener noreffer">stable identifier to attach storage</a>
. All containers of ECS would utilize the same volume of EFS. This means we cannot simply tell our program to use the <code>/data/badger</code> directory. When Badger specifies a directory to <code>Open</code>, it acquires a lock on that directory. This means no other process may modify the directory while the instance of Badger is holding a lock. We can use this to our benefit by passing a unique id to the directory path for the Badger database. e.g., <code>/data/badger/instance-fe1ed453-c964-45dc-a5a6-ff6959b60dd9</code>. Now every instance of ECS will have it&rsquo;s own unique Badger database on our EFS volume.</p><p>The problem left is reaping zombie Badger databases when an ECS container shuts down. Badger was created specifically for the <a href=https://github.com/dgraph-io/dgraph target=_blank rel="noopener noreffer">Dgraph</a>
database. One of the things Badger does exceptionally well is copy its contents by <a href=https://github.com/dgraph-io/badger#stream target=_blank rel="noopener noreffer">streaming</a>
it out to another instance. Using this streaming process, we can have a background goroutine running on every instance of our service that periodically tries to acquire locks on the other Badger data directories. If the background goroutine is able to successfully acquire a lock on another directory, that means the ECS instance which created that Badger directory has shut down. At this point, we can stream the contents of that directory&rsquo;s database into the one actively being managed by this instance. By automatically reaping zombie directories, this will allow us to scale our ECS instances up and down automatically without having to do any kind of managed clustering.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=c1>// loop over other instance directories.
</span><span class=c1>// e.g.,
</span><span class=c1>// /data/badger/instance-fe1ed453-c964-45dc-a5a6-ff6959b60dd9
</span><span class=c1>// /data/badger/instance-392358a9-1d61-4a5c-86b9-1a8059f25dc8
</span><span class=c1>// /data/badger/instance-89ce45ff-f925-4c50-9cec-677b06f475d9
</span><span class=c1></span>
<span class=c1>// When we get a lock on one, stream the contents of that database into
</span><span class=c1>// the database we created when the service started.
</span><span class=c1>// 89ce45ff-f925-4c50-9cec-677b06f475d9 -&gt; fe1ed453-c964-45dc-a5a6-ff6959b60dd9
</span><span class=c1></span>
<span class=c1>// Finally, remove the database we just copied.
</span></code></pre></td></tr></table></div></div><p><em>Requirement #5 - We need acknowledgment capabilities between every producer, consumer, and webhook.</em></p><p>We&rsquo;ve already touched on this requirement quite a bit. Any time a producer produces a message, that message must be acknowledge once it&rsquo;s been successfully processed between the end to end systems.</p><p>Let&rsquo;s circle back to Requirement #2. Now that we have our disk storage figured out by wiring up NATS to Badger, we have a way to guarantee messages are persisted to disk and will survive system restarts. When a message is destined for a webhook subject and that webhook does not respond in a reasonable amount of time, we can simply write it out to our dead letter queue. Let&rsquo;s see what that looks like.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-go data-lang=go><span class=kd>func</span> <span class=nf>sendToSaasWebhook1</span><span class=p>(</span><span class=nx>msg</span> <span class=o>*</span><span class=nx>nats</span><span class=p>.</span><span class=nx>Msg</span><span class=p>)</span> <span class=kt>error</span> <span class=p>{</span>
    <span class=nx>err</span> <span class=o>:=</span> <span class=nf>webhook1HttpRequest</span><span class=p>(</span><span class=nx>msg</span><span class=p>)</span>
    <span class=k>if</span> <span class=nx>err</span> <span class=o>!=</span> <span class=kc>nil</span> <span class=p>{</span>
        <span class=c1>// Publish the message to our DLQ subject.
</span><span class=c1></span>        <span class=nf>publishToDLQ</span><span class=p>(</span><span class=nx>msg</span><span class=p>)</span>
        <span class=k>return</span> <span class=nx>err</span>
    <span class=p>}</span>
    <span class=c1>// Success writing to SaaS webhook.
</span><span class=c1></span>    <span class=k>return</span> <span class=kc>nil</span>
<span class=p>}</span>
</code></pre></td></tr></table></div></div><p>We&rsquo;ll design our dead letter queue so that it will automatically retry sending messages it receives. When it receives a message, the message will be persisted to Badger with a key that is unique and sortable. The key will internally contain a timestamp which will allow us to sort the keys by time. This means we can have a goroutine that runs in a loop looking for messages in Badger that are ready to be republished. Any timestamps earlier than the current time will signal that the message is ready to be republished back to the original subject. Doing this allows us to delay the republishing of messages by creating keys with embedded timestamps that occur in the future. If we wanted to try republishing a message 15 minutes from now, we would create a key with an embedded timestamp of 15 minutes from now. When the 15 minutes have passed, the current time will be greater than the key timestamp and the message will be picked up and republished.</p><p><figure><img src=/svg/loading.min.svg data-sizes=auto data-src=requeue.svg alt="Diagram of a Retry Service" title="Diagram of a Retry Service" class=lazyload><figcaption class=image-caption>Diagram of a Retry Service</figcaption></figure></p><p>Instead of adding a Streaming technology between every producer and consumer, we have instead opted to allow producers to publish at any rate they wish. In the event the consumer is not able to keep up, the producer falls back to sending the message to our Retry Service, also known as Requeue. Because our Retry Service is running in ECS it will automatically scale to the needs of our publishers. Since our Retry Service consumers are all part of the same queue group, NATS will load balance messages across the Retry Service instances. Upon receiving a message, the Retry Service will persist it to the Badger database on disk, using a key with a timestamp at some point in the future. When the time comes, that message will be picked up and republished back on the subject it was intended for. If the consumer, such as <code>webhook-1-consumer</code> still does not acknowledge the message, exponential backoff will come into play and the message will be retired again at some point in the future. This process may continue until <code>webhook1</code> comes back online and the message is successfully delivered to the webhook.</p><h3 id=decoupling>Decoupling</h3><p>You&rsquo;ll notice we&rsquo;re not making a connection to the Retry Service and webhook consumers directly. Instead of tightly coupling our producers and consumers by connecting them over something like gRPC, we&rsquo;re able to make connections to our NATS cluster and reuse those connections for our publishing and subscribing needs. This makes doing things like testing much simpler because we don&rsquo;t have to replicate our entire networking and infrastructure stack, instead, we can embed a NATS server in our tests. Additionally, refactoring and reuse of code become easier, allowing us to move publisher and subscriber code around as we see fit without the constraints of our network topology between services.</p><h3 id=enhancements>Enhancements</h3><p><strong>Backpressure</strong></p><p>Before I wrap things up, I want to talk about a few enhancements we can make to our service. For starters, the Retry Service allows us to handle backpressure automatically. When a producer starts to receive backpressure due to it sending messages faster than the consumers can acknowledge, it can fall back to the Retry Service which should autoscale. The Retry Service will then attempt to publish the messages again at some point in the future when the consumer is better able to handle the messages.</p><p><strong>Rate Limiting</strong></p><p>Rate limiting is something our consumers may have to deal with when writing to a webhook. Most SaaS APIs will specify a rate limit for each customer to ensure that a single customer does not overload the API for all customers. In the event that an API does reject messages due to a rate limit, it may send back an HTTP header specifying the amount of time the customer should wait before retrying. If this is the case, instead of a consumer acknowledging back to a producer that the message was successfully processed, it may instead opt to send back a message specifying that the message should be retried in the amount of time provided by the webhook response. The producer can, in this case, offload the message to our retry service, with a payload specifying how long the retry service should wait before trying to publish the message again.</p><p><strong>Time-To-Live and Expiration</strong></p><p>Sometimes for whatever reason, messages may end up in a permanent failure situation. Maybe the payload of the message is malformed and it will never succeed in posting to a webhook endpoint. In this case, we don’t want the message to sit around in our Badger database, retrying over and over until the end of time. What we can do here is specify a maximum amount of time the message should be kept in Badger. This value, known as a TTL, can be set when we commit the message to Badger. Periodically, Badger will look for messages with an expired TTL and delete them for us. We can take this one step further and upon publishing messages, allow producers to specify TTLs for messages in the message payload. This gives each producer the ability to determine how long a message should be kept in the Retry Service before being removed.</p><h3 id=summary>Summary</h3><p>Hopefully, you can see the power of this pattern. By offloading any messages that are not acknowledged to the auto-scaling Retry Service, we ensure we don&rsquo;t lose messages. We are able to code our producers in a way that allows them to fall back onto the Retry Service, which will automatically take care of retrying the publishing messages to their original topic at a later time. With this pattern, any number of topics can be created with any number of consumers without adding any additional Streaming technologies between our producers and consumers. I’ve put together some code for what a retry service like this might look like. Feel free to <a href=https://github.com/nickpoorman/nats-requeue target=_blank rel="noopener noreffer">check it out on GitHub</a>
.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>The article was updated on 2020-07-27</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=https://nickpoorman.com/2020-07-26-data-pipelines-part-2-retries/index.md target=_blank>Read Markdown</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://nickpoorman.com/2020-07-26-data-pipelines-part-2-retries/ data-title="Data Pipelines: Part 2 - Retries" data-via=nickpoorman data-hashtags="Golang,Data,Queuing,Messaging Patterns,NATS,gRPC,Retries"><i class="fab fa-twitter fa-fw"></i></a><a href=javascript:void(0); title="Share on Linkedin" data-sharer=linkedin data-url=https://nickpoorman.com/2020-07-26-data-pipelines-part-2-retries/><i class="fab fa-linkedin fa-fw"></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://nickpoorman.com/2020-07-26-data-pipelines-part-2-retries/ data-title="Data Pipelines: Part 2 - Retries"><i class="fab fa-hacker-news fa-fw"></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://nickpoorman.com/2020-07-26-data-pipelines-part-2-retries/><i class="fab fa-reddit fa-fw"></i></a></span></div></div></div><div class=post-info-more><section><span><a href=/tags/golang><i class="fas fa-tag fa-fw"></i>Golang</a>
</span>&nbsp;<span>
<a href=/tags/data><i class="fas fa-tag fa-fw"></i>Data</a>
</span>&nbsp;<span>
<a href=/tags/queuing><i class="fas fa-tag fa-fw"></i>Queuing</a>
</span>&nbsp;<span>
<a href=/tags/messaging-patterns><i class="fas fa-tag fa-fw"></i>Messaging Patterns</a>
</span>&nbsp;<span>
<a href=/tags/nats><i class="fas fa-tag fa-fw"></i>NATS</a>
</span>&nbsp;<span>
<a href=/tags/grpc><i class="fas fa-tag fa-fw"></i>gRPC</a>
</span>&nbsp;<span>
<a href=/tags/retries><i class="fas fa-tag fa-fw"></i>Retries</a>
</span>&nbsp;</section><section><span><a href=javascript:window.history.back();>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=https://nickpoorman.com/2020-07-26-data-pipelines-part-1-queuing-and-messaging-patterns/ class=prev rel=prev title="Data Pipelines: Part 1 - Queuing and Messaging Patterns"><i class="fas fa-angle-left fa-fw"></i>Data Pipelines: Part 1 - Queuing and Messaging Patterns</a></div></div><div class=comment></div></article></div></main><footer class=footer><div class=copyright><div class=copyright-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2020</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>Nick Poorman</a></span></div></div></footer></div><a href=# class="dynamic-to-top animated faster" id=dynamic-to-top><span>&nbsp;</span>
</a><script>function _loadKatex(){}</script><script async src=/js/bundle-scriptLocal.min.9c14e4a929cb9597b5f98895cce6ed350edb633e3ce7f2bf5b8949ea9106e7bf8e7b540145bd24582018941f470378e9c3f76bb8f747d8f181fdee129509300a.js integrity="sha512-nBTkqSnLlZe1+YiVzObtNQ7bYz485/K/W4lJ6pEG57+Oe1QBRb0kWCAYlB9HA3jpw/druPdH2PGB/e4SlQkwCg=="></script><script async defer data-domain=nickpoorman.com src=https://t.nickpoorman.com/js/index.js></script></body></html>